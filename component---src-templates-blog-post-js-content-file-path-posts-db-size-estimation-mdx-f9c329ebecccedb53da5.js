"use strict";(self.webpackChunkgatsby_starter_default=self.webpackChunkgatsby_starter_default||[]).push([[855],{5140:function(e,t,n){n.r(t),n.d(t,{default:function(){return d}});var l=n(2333),a=n(9474);function r(e){const t=Object.assign({p:"p",h2:"h2",blockquote:"blockquote",ul:"ul",li:"li",strong:"strong"},(0,l.R)(),e.components);return a.createElement(a.Fragment,null,a.createElement(t.p,null,'Ever been in the hot seat when a DBA asks, "What\'s your DB size going to be in a year?" ðŸ˜Ž'),"\n",a.createElement(t.p,null,"At Agoda, we traditionally relied on SQL Server for our app. But let's face it, SQL Server can be a pricey beast. So, we decided to embrace the happy elephant, PostgreSQL. Why? That's a story for another day."),"\n",a.createElement(t.p,null,"I was tasked with designing a highly scalable, event-driven booking search system. To give you a sense of scale, Agoda processes a few hundred bookings daily (let's assume 200k ðŸ˜Ž)â€”which means a few bookings every second!"),"\n",a.createElement(t.p,null,"Since PostgreSQL was new to our platform, our cautious DBA team didn't want to risk it with a mission-critical service exceeding 300 GB. The burning question: Will my service exceed this limit?"),"\n",a.createElement(t.p,null,"The expectation was to have at least two years' worth of historical booking data ready at launch."),"\n",a.createElement(t.h2,null,"The ChatGPT Consultation"),"\n",a.createElement(t.p,null,"Like any newbie, I turned to ChatGPT and asked:"),"\n",a.createElement(t.blockquote,null,"\n",a.createElement(t.p,null,'"If my PostgreSQL DB hosts a single table with a max of ten columns, one row per booking, and we get around 200k bookings per day, what\'s the data size for 2 years?"'),"\n"),"\n",a.createElement(t.p,null,"ChatGPT's short answer: Yes, it will exceed 300 GB, with each row being 1KB. Yikes, that's hefty!"),"\n",a.createElement(t.p,null,"But wait, how can a single row be 1KB?"),"\n",a.createElement(t.h2,null,"The Initial Calculation"),"\n",a.createElement(t.p,null,"After some digging, I calculated the average row size:"),"\n",a.createElement(t.ul,null,"\n",a.createElement(t.li,null,"10 columns total"),"\n",a.createElement(t.li,null,"1 bigint (8 bytes)"),"\n",a.createElement(t.li,null,"3 integers (12 bytes)"),"\n",a.createElement(t.li,null,"5 timestamps (40 bytes)"),"\n",a.createElement(t.li,null,"1 int[] (assuming an average of 5 ints, 20 bytes)"),"\n"),"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Total estimated row size: 80 bytes.")),"\n",a.createElement(t.p,null,"For two years, that's 146,000,000 rows Ã— 80 bytes/row = ",a.createElement(t.strong,null,"11.68 GB"),"."),"\n",a.createElement(t.p,null,"12 GB? Sweet! I got the green light to onboard to PostgreSQL, and my app went live."),"\n",a.createElement(t.h2,null,"Reality Check"),"\n",a.createElement(t.p,null,"But was I right? After all, I'm a rookie, and it's just me and AIâ€”a perfect evil combo, right? ðŸ¤£"),"\n",a.createElement(t.p,null,"While I wasn't too far off, I revisited the week's data."),"\n",a.createElement(t.p,null,"The current DB size as I write this is ",a.createElement(t.strong,null,"780 MB")," for ",a.createElement(t.strong,null,"5,056,637 rows")," (I haven't hydrated the historical data yet). That doesn't add upâ€”5,056,637 rows Ã— 80 bytes should be 404 MB. Why am I off by almost 380 MB, nearly 50% more?"),"\n",a.createElement(t.h2,null,"The Missing Pieces"),"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Surprise!")," The index size is 162 MB, which I didn't factor in. But there are still 220 MB unaccounted for."),"\n",a.createElement(t.p,null,"Ah, the average row size turned out to be ",a.createElement(t.strong,null,"130 bytes"),". I also overlooked row overhead (fields reserved for transaction management, tuple headers, TOAST), which is a minimum of 30 bytes."),"\n",a.createElement(t.h2,null,"The Corrected Calculation"),"\n",a.createElement(t.p,null,"Considering these factors, recalculating gives:"),"\n",a.createElement(t.ul,null,"\n",a.createElement(t.li,null,"146,000,000 rows Ã— 130 bytes/row â‰ˆ ",a.createElement(t.strong,null,"20 GB")),"\n",a.createElement(t.li,null,"Plus 20% for indexes â‰ˆ ",a.createElement(t.strong,null,"24 GB")),"\n"),"\n",a.createElement(t.p,null,"That's more than double my initial estimate."),"\n",a.createElement(t.h2,null,"Lessons Learned"),"\n",a.createElement(t.p,null,"It's been a valuable learning experience for me, and I hope it is for you too."),"\n",a.createElement(t.p,null,"These basics might just save you from a hefty AWS RDS bill someday."),"\n",a.createElement(t.p,null,a.createElement(t.strong,null,"Key takeaways:")),"\n",a.createElement(t.ul,null,"\n",a.createElement(t.li,null,"Always account for row overhead (minimum 30 bytes in PostgreSQL)"),"\n",a.createElement(t.li,null,"Don't forget about index storage requirements"),"\n",a.createElement(t.li,null,"Real-world row sizes are often larger than theoretical calculations"),"\n",a.createElement(t.li,null,"Test with actual data when possible"),"\n"),"\n",a.createElement(t.p,null,"Comments are most welcome! How do you estimate your DB size? ðŸ˜"))}var o=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,l.R)(),e.components);return t?a.createElement(t,e,a.createElement(r,e)):r(e)},s=n(3328),i=n(9389),c=n(4568),u=n(910);const m={pre:c.N,code:c.R},h=e=>{let{data:t,location:n,children:r}=e;const o=t.mdx;return a.createElement(i.A,{location:n,headerTitle:"Fasil Marshooq",title:o.frontmatter.title,description:o.frontmatter.description||o.excerpt,date:o.frontmatter.date,tags:o.frontmatter.tags},a.createElement("div",{className:s.Qs,id:"md-content",itemProp:"articleBody"},a.createElement(l.x,{components:m},r),a.createElement(u.A)))};function d(e){return a.createElement(h,e,a.createElement(o,e))}}}]);
//# sourceMappingURL=component---src-templates-blog-post-js-content-file-path-posts-db-size-estimation-mdx-f9c329ebecccedb53da5.js.map