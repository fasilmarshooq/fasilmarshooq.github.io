{"data":{"allMdx":{"nodes":[{"id":"05049133-a849-5d76-886b-66ae32eea76f","frontmatter":{"title":"A Rookie System Designer's Journey - Estimating DB Size","date":"December 19, 2024","tags":["tech","database","postgresql","system-design"]},"fields":{"slug":"/db-size-estimation/"},"body":"\nEver been in the hot seat when a DBA asks, \"What's your DB size going to be in a year?\" 😎 \n\nAt Agoda, we traditionally relied on SQL Server for our app. But let's face it, SQL Server can be a pricey beast. So, we decided to embrace the happy elephant, PostgreSQL. Why? That's a story for another day.\n\nI was tasked with designing a highly scalable, event-driven booking search system. To give you a sense of scale, Agoda processes a few hundred bookings daily (let's assume 200k 😎)—which means a few bookings every second!\n\nSince PostgreSQL was new to our platform, our cautious DBA team didn't want to risk it with a mission-critical service exceeding 300 GB. The burning question: Will my service exceed this limit?\n\nThe expectation was to have at least two years' worth of historical booking data ready at launch.\n\n## The ChatGPT Consultation\n\nLike any newbie, I turned to ChatGPT and asked:\n\n> \"If my PostgreSQL DB hosts a single table with a max of ten columns, one row per booking, and we get around 200k bookings per day, what's the data size for 2 years?\"\n\nChatGPT's short answer: Yes, it will exceed 300 GB, with each row being 1KB. Yikes, that's hefty!\n\nBut wait, how can a single row be 1KB?\n\n## The Initial Calculation\n\nAfter some digging, I calculated the average row size: \n- 10 columns total\n- 1 bigint (8 bytes)\n- 3 integers (12 bytes) \n- 5 timestamps (40 bytes)\n- 1 int[] (assuming an average of 5 ints, 20 bytes)\n\n**Total estimated row size: 80 bytes.**\n\nFor two years, that's 146,000,000 rows × 80 bytes/row = **11.68 GB**.\n\n12 GB? Sweet! I got the green light to onboard to PostgreSQL, and my app went live.\n\n## Reality Check\n\nBut was I right? After all, I'm a rookie, and it's just me and AI—a perfect evil combo, right? 🤣 \n\nWhile I wasn't too far off, I revisited the week's data.\n\nThe current DB size as I write this is **780 MB** for **5,056,637 rows** (I haven't hydrated the historical data yet). That doesn't add up—5,056,637 rows × 80 bytes should be 404 MB. Why am I off by almost 380 MB, nearly 50% more?\n\n## The Missing Pieces\n\n**Surprise!** The index size is 162 MB, which I didn't factor in. But there are still 220 MB unaccounted for. \n\nAh, the average row size turned out to be **130 bytes**. I also overlooked row overhead (fields reserved for transaction management, tuple headers, TOAST), which is a minimum of 30 bytes.\n\n## The Corrected Calculation\n\nConsidering these factors, recalculating gives:\n- 146,000,000 rows × 130 bytes/row ≈ **20 GB**\n- Plus 20% for indexes ≈ **24 GB**\n\nThat's more than double my initial estimate.\n\n## Lessons Learned\n\nIt's been a valuable learning experience for me, and I hope it is for you too.\n\nThese basics might just save you from a hefty AWS RDS bill someday.\n\n**Key takeaways:**\n- Always account for row overhead (minimum 30 bytes in PostgreSQL)\n- Don't forget about index storage requirements\n- Real-world row sizes are often larger than theoretical calculations\n- Test with actual data when possible\n\nComments are most welcome! How do you estimate your DB size? 😁 "},{"id":"84c7f1e0-cd11-5792-be99-dfe6031260e3","frontmatter":{"title":"Why Software Engineers Should Brush Up on Physics","date":"December 19, 2024","tags":["tech","distributed-systems","system-design"]},"fields":{"slug":"/physics-for-software-engineers/"},"body":"\nLast week, our team was deep in the trenches, working on a multi-DC event-driven system designed to power booking searches.\n\nAs part of the process, I was testing replication delays between our data centers in Hong Kong and Ashburn. The dashboard proudly displayed a maximum delay of **17ms**.\n\nCue one of our lead engineers chiming in: \"Anything under 100ms is physically impossible.\"\n\nMy confident response? \"Trust me, bro.\" 😅\n\nThen came the mic-drop moment from my director:\n\n> \"I trust the speed of light more than you.\"\n\n**Big facepalm moment.**\n\n## Let's Do the Math\n\nThe distance between Hong Kong and Ashburn is about **13,100 km**.\n\nLight travels at **300,000 km/s**.\n\nIn theory (vacuum conditions), it would take roughly **45ms** for light to cover that distance.\n\n```\nDistance: 13,100 km\nSpeed of light: 300,000 km/s\nTheoretical minimum: 13,100 ÷ 300,000 = 0.044 seconds = 44ms\n```\n\nAdd in real-world factors (fiber-optic cables, signal processing delays, and the absence of a magic vacuum tube), and even 45ms is optimistic.\n\n## The Reality Check\n\nSo, yeah... that 17ms? Either our dashboard is defying the laws of physics, or we've just rewritten them. (I'll let you guess which.) 😅"},{"id":"2694c0f4-a3ab-5cb8-a495-43997f57a974","frontmatter":{"title":"NetPad - A Handy Multi-platform Tool for C# Developers","date":"November 01, 2024","tags":["tech","tools","csharp"]},"fields":{"slug":"/netpad/"},"body":"I have extesively used [LINQPad](https://www.linqpad.net/purchase.aspx) during my early days of learning C# and .Net. This nifty tool to run C# code snippets and see the output. Its a freemium product and it was fantastic to use. Fast forward a few years, and a colleague at Agoda reintroduced me to LINQPad. I was blown away by the new features and how much it had evolved. And guess what? We had access to the premium version! You can check out all the cool features of LINQPad [here](https://www.linqpad.net/purchase.aspx).\n\nThis blog isn't about LINQPad—we all know it's a fantastic tool. But here's the catch: to use LINQPad, you need a Windows machine. Our beloved .NET is cross-platform now, and many of our engineers are using Macs for .NET development. So, I went on the hunt for a tool that can run C# code snippets, and I found [NetPad](https://github.com/tareqimbasher/NetPad). Just to be clear, it's not a LINQPad alternative, but it can run C# code snippets and offers much more.\n\n![netpad-reddit](./images/netpad/netpad-reddit.png)\n\n<br />\nKudos to the creator of NetPad, [Tareq](https://www.linkedin.com/in/tareq-imbasher/)! NetPad boasts a VS Code-like UI and supports one of LINQPad's premium features: [importing NuGet packages](https://www.linqpad.net/Purchase.aspx#NuGet). Plus, it has excellent IntelliSense support.\n\nLet's explore a couple of amazing use cases for NetPad.\n\n### Running Code Snippets with NuGet Packages\n\nYou can literally run any C# code snippet and see the output. Here, I'm trying to create a Kafka listener and check the output. You can add the NuGet package to the context by clicking on the wrench symbol or pressing the `F4` key. This opens the Package Manager window, where you can add NuGet packages. After that, it's business as usual: write your code and run it.\n\n![netapad-nugget](./images/netpad/netpad-nugget.png)\n\n<br />\nAs you can see from the image below, I've added the NuGet package `Confluent.Kafka`, created a Kafka listener, and ran the code. The output is displayed in the output window.\n\n![netapad-output](./images/netpad/netpad-kafka2.png)\n\n<br />\nCurious about what the `</>code` window does? It gives you the AST (Abstract Syntax Tree) of the code you've written. The AST provides the structure of your code, helping you understand how the IDE reads it. You can use this to create analyzers or code fixers.\n\n![netapad-code](./images/netpad/netpad-code.png)\n\n<br />\n\n### Connecting to a Database with Windows Authentication\n\nYou can also connect to a database and run queries. Here, I'm connecting to a SQL Server using Windows authentication. To support Windows auth, just add `Integrated Security=True;` to your custom connection string. The maintainer has agreed to include this option in the UI in the next release.\n\n![netpad-wind](./images/netpad/netpad-wind.png)\n<br />\nOnce saved, it will take a few seconds to scaffold the database, and then you can run your queries. You can either right-click on the connection and select \"Use in new script\" or just select the connection from the script pane. Here's a basic query execution:\n\n![netpad-basicquery](./images/netpad/netpad-basicquery.png)\n<br />\nIf you're a big fan of LINQPad's `.Dump()`, NetPad's version is pretty basic, but it gets the job done. For those curious about how LINQPad's dump works, check out the image below.\n\n![linpad-dump](./images/netpad/linpad-dump.png)\n<br />\nYou see how it gives you nice formatting of relationship mapping? It's incredibly helpful for large databases. Hopefully, NetPad will include this feature in the next release.\n\nNetPad also lets you visualize the SQL executed, which is pretty handy for understanding complex LINQ queries.\n\n![netpad-sql](./images/netpad/netpad-sql.png)\n<br />\n\nThe ability to execute LINQ code while connected to a database is pretty cool. Whether you're patching the database for complex data fixes, producing ad-hoc Kafka events based on data during debugging, or any other use case you can imagine, it's incredibly useful.\n\nNetPad is still in its early days and has a long way to go, but it's already a great tool to have in your toolkit.\n"},{"id":"3ee35197-9e68-5605-ad46-0fedba1c8961","frontmatter":{"title":"Streamline App Monitoring with Seq","date":"September 21, 2024","tags":["tech","monitoring"]},"fields":{"slug":"/seq-monitoring/"},"body":"\nAs your applications scale, monitoring becomes a critical aspect of maintaining their health. Developers despise manual monitoring and logging into dashboards to hunt for anomalies.\n\nLet's build a monitoring and alerting system using open-source tools.\n\nThere are countless monitoring tools available, but we're going to use Seq because it's simple, freemium, and can be easily run using Docker to start collecting logs and events. Plus, it adheres to OpenTelemetry (OTel) standards, making it compatible with any platform.\n\nImagine we have an e-commerce application. We need to monitor and alert when the overall error rate spikes and keep an eye on user visits, a key metric for our business.\n\nLet's dive in and install Seq using Docker.\n\n```bash\ndocker run \\\n  --name seq \\\n  -d \\\n  --restart unless-stopped \\\n  -e ACCEPT_EULA=Y \\\n  -e SEQ_FIRSTRUN_ADMINPASSWORDHASH=\"$PH\" \\\n  -v <local path to store data>:/data \\\n  -p 5341:80 \\\n  datalust/seq\n```\n<br/>\n\nThis should start Seq on port 5341. You can access the dashboard at `http://localhost:5341`. If you run into any issues, refer to the official [documentation](https://docs.datalust.co/docs/getting-started-with-docker).\n\nSeq offers SDKs for most platforms and adheres to OpenTelemetry (OTel) standards, so you can use any OTel SDK to send logs to Seq. Check out this [link](https://docs.datalust.co/v4/docs/using-serilog) for integrating Seq as a Serilog sink, with examples for nearly all platforms.\n\nOnce your logs are flowing in, you can create alerts on the Seq dashboard based on any log pattern or metric. By default, Seq provides a dashboard displaying essentials like error rates, log counts, and event counts. You can also create custom dashboards to suit your needs.\n\n![dashboard](./images/seq/dashboard.png)\n<br/>\n\nLet's be honest, dashboards are great, but what we really need are alerts that keep us informed without constant monitoring.\n\nTo create an alert, head over to the `Alerts` tab and click the `Add Alert` button.\n\nGive your alert a meaningful name, choose the signal that should trigger it, and craft your query. You can even execute the query to ensure it triggers the alert as expected.\n\n![create-alert](./images/seq/create-alert.png)\n<br/>\n\nI crafted this query to trigger an alert when the error count exceeds 50 within a 30-minute window. This way, we can catch issues before they escalate.\n\n```sql\nselect count(*) as count\nfrom stream\ngroup by time(30m)\nhaving count > 50\nlimit 100\n```\n<br/>\n\nNow that we've set up an alert, it's time to create a notification channel. I prefer notifications to be delivered to the app our team uses the most, like Slack, Teams, or even Telegram.\n\nFor this example, we'll use Slack. You can create a Slack app and use a webhook to send notifications. Check out this [blog](../create-slack-bot) for a step-by-step guide on creating a Slack bot.\n\nOnce you have the webhook, let's install the Slack app in Seq. Head over to settings in Seq, click on `Apps`, then hit the `INSTALL FROM NUGET` button. Enter `Seq.App.Slack` as the package ID and click install. You should now see the Slack app listed in the apps tab.\n\n![slack-seq](./images/seq/slack-seq.png)\n<br/>\n\nNow, let's wire up the webhook in the app. Click on the `ADD INSTANCE` button on the installed Slack app, provide the webhook URL, and hit save. No need to worry about other details—they'll be automatically inferred from the webhook.\n\nWith the Slack app set up, head back to the alert, navigate to the `Notifications` section, set the level to `Error`, select the Slack app we just installed, and click save.\n\n![noti](./images/seq/noti.png)\n<br/>\n\nVoilà! Now, whenever the alert is triggered, you'll receive a Slack notification, keeping you in the loop without having to constantly check the dashboard.\n\n![high-error](./images/seq/high-error.png)\n<br/>\n\nSimilarly, you can count the number of events logged when a user visits your site and create an alert for that. Trust me, these kinds of alerts can save your business from losing money. Customers won't complain—they'll simply leave.\n\nBut remember, don't create too many alerts that generate noise. Focus on key metrics that truly impact your business.\n\nHappy Monitoring!\n\nIf you enjoyed this blog, show some love by leaving a comment below and sharing it with someone who might find it useful.\n\n\n\n\n"},{"id":"040b9dde-02a5-564c-84c6-5e52ee75c681","frontmatter":{"title":"Slack Bot - A Simple and Straightforward Guide","date":"September 17, 2024","tags":["tech","slack"]},"fields":{"slug":"/create-slack-bot/"},"body":"\nEver felt like the official Slack documentation was written in hieroglyphics? You're not alone! After spending way too much time deciphering how to create a Slack bot and enable webhooks, I decided to put together this guide.\n\nWhy? Because getting real-time notifications in your Slack channel shouldn't be rocket science. Plus, my team and I use Slack for almost everything, so having those event notifications right where we chat is a game-changer.\n\nSo, let's cut through the noise and get you set up with a Slack bot in no time!\n\n\n![deploy](./images/slackbot/slack-deploy.png)\n\n<br />\n![issue](./images/slackbot/slack-issue.png)\n\n<br />\n\nLet's dive into how we can create these awesome notifications!\n\nHead over to the [Slack API](https://api.slack.com/apps). Once you're logged in, click on the `Create New App` button.\n\n![createnew](./images/slackbot/create-new.png)\n\n<br />\n\nClick on `Create from scratch` to get started with your new Slack bot.\n\n![scratch](./images/slackbot/scratch.png)\n\n<br />\n\nGive your app a catchy name, select the workspace where it will live, and hit `Create App`.\n\n![naming](./images/slackbot/naming.png)\n\n<br />\n\nNow that we've created the app, let's set up a webhook to publish messages to a specific channel.\n\nClick on `Incoming Webhooks` and toggle the `Activate Incoming Webhooks` switch. This will enable the `Add new webhook to workspace` button.\n\n![addwh](./images/slackbot/add-new-wh.png)\n\n<br />\n\nHeads up: If you're on Slack's free tier, you're limited to 10 apps per workspace. So, make sure you only install what's necessary.\n\nNext, select the channel where your bot should post. I've created a channel called `#test-bot` for testing purposes.\n\n![select-channel](./images/slackbot/select-channel.png)\n\n<br />\n\nOnce you click on `Allow`, you'll be redirected back to the webhooks page where you'll see your shiny new webhook.\n\n![created-hook](./images/slackbot/created-hook.png)\n\n<br />\n\nNow, let's test it out! Replace `<your awesome webhook>` in the curl command below with your actual webhook URL:\n\n```curl\ncurl --location '<your awesome webhook>' \\\n--header 'Content-type: application/json' \\\n--header 'Cookie: b=5302b6a0d86156be293aae2e14ef0cd1' \\\n--data '{ \"text\": \":60fps_parrot: :60fps_parrot: :60fps_parrot: Great Notification!\"}'\n```\n\n<br />\nMake a curl request via terminal or from Postman, and voilà!\n\n![test-noti](./images/slackbot/test-noti.png)\n\n<br />\n\nYou can further customize how the bot looks in the app management page.\n\nIn my next blog, [Streamline App Monitoring with Seq](../seq-monitoring), I'll discuss how we leverage Slack for app monitoring.\n\nStay tuned!\n"},{"id":"6d81b30b-3946-5e63-aa41-206dcc438d18","frontmatter":{"title":"PNPM - Speed up your package installation","date":"September 15, 2024","tags":["tech","JavaScript"]},"fields":{"slug":"/pnpm/"},"body":"\nNode modules are my beloved feature in the JavaScript ecosystem. But they're also the most hated, thanks to their labyrinthine resolution process. Once upon a time, npm install was a coffee break affair, but as projects grew, it turned into a full-blown lunch break. Enter Yarn, our knight in shining armor! We adored Yarn's package resolution magic. Yet, even in 2024, despite npm's glow-up, I still can't shake the feeling. It's like Internet Explorer—great for downloading Chrome. Similarly, npm's main gig for me is installing Yarn.\n\nBut then, PNPM swooped in and flipped the script. Sure, Yarn sped up installations and improved caching, but PNPM took it to the next level. Take lodash, for instance—a staple in nearly every project. Why should I have to install and resolve it in every single one? With PNPM, it's like having a communal pantry: install it once, and symlink it everywhere. Genius!\n\n![pnpm-meme](./images/pnpm/pnpm-meme.png)\n\n<br />\n\nPNPM's installation process is streamlined into three efficient stages. First, it identifies and fetches all dependencies to a central store. Then, it calculates the optimal node_modules directory structure. Finally, it links everything from the store to node_modules. This method is much faster than the traditional approach of resolving, fetching, and writing all dependencies directly to node_modules, making installations quicker and more efficient.\n\n![pnpm-resolve](./images/pnpm/pnpm-resolve.png)\n\n### What About CI?\n\nIn the realm of CI, Many find that directly installing dependencies with PNPM can be faster than the traditional caching and downloading of `node_modules`, thanks to its efficient dependency management. However, experiences may vary depending on the project. [You can also cache using PNPM](https://pnpm.io/continuous-integration#github-actions). For those looking to optimize further, consider setting up a custom runner preloaded with frequently used node modules—it's like having a well-stocked pantry, ready to streamline your build process.\n\nEnough philosophy lets see how to move to pnpm easily from npm or yarn.\n\n1. Install PNPM\n\n```bash\nnpm install -g pnpm\n```\n\n<br />\n\n2. Import from current package manager to creatre pnpm-lock.yml\n\n```bash\npnpm import\n```\n\n<br />\n\n3. Remove existing node_module , you can delete them manually or you can use npkill tool.\n\n```bash\nnpx npkill\n```\n\n<br />\n\n4. install dependencies\n\n```bash\npnpm i\n```\n\n<br />\n\nTada!!!! now you are all set to use pnpm, lets do some additional steps to cleanup\n\n5. Remove old lock files\n\n```bash\nrm yarn-lock.json\n```\n\n<br />\n\n6. Prevennt other from using yarn or node, Add this to your package.json\n\n```json\n\"scripts\": {\n  \"preinstall\": \"npx only-allow pnpm\",\n  ...\n}\n```\n\n<br />\n\n---\n\n### Important Note! - Dependencies in PNPM Are Not Hoisted\n\nWith npm or Yarn Classic, all packages end up at the root of the `node_modules` directory. This setup allows your code to access packages that aren't directly listed as dependencies, which can lead to unexpected behavior.\n\nPNPM, on the other hand, uses symlinks to place only the project's direct dependencies at the root. This approach keeps your `node_modules` directory clean and organized, ensuring that your project only accesses the dependencies it explicitly declares.\n\nWhile most packages will work fine with this setup, you might occasionally need to install additional dependencies manually. If you encounter issues, you can enable hoisting by setting [shamefully-hoist to true](https://pnpm.io/npmrc#shamefully-hoist) in your configuration. This will mimic the behavior of npm and Yarn Classic by placing all packages at the root.\n"},{"id":"2d66f97b-a48d-5e04-ab69-ea4ba65e5110","frontmatter":{"title":"TOTP - MFA made simple","date":"September 12, 2024","tags":["tech",".Net","security"]},"fields":{"slug":"/totp/"},"body":"\nBasic Authentication (just username and password) has become increasingly insecure, as it can be easily compromised through phishing, brute force attacks, or data breaches. Let's face it: all systems have security flaws, but we should make it difficult for attackers.\n\nSo, what can we do? Well, we can do many things, but the easiest go-to answer is \"Multi-Factor Authentication\" (MFA).\n\n![dog-mfa](./images/mfa-dog.png)\n\n<br />\nAttention 🚨 - MFA is not a replacement for allowing users to create weak passwords.\n\nThere are multiple ways to enable MFA. Let's explore a few of them.\n\n### Email Based OTP\n\nAn OTP will be delivered to the registered email. Accessibility is a pro here, as most users have an email account. However, you need to set up SMTP, store the OTP to validate it later, and be aware that emails can be compromised.\n\n### Sms Based Otp\n\nAn OTP will be delivered to the registered mobile number. It's easy to use and poses a lesser security risk than email-based OTPs. But... there are high costs involved.\n\nSo, this leaves us to select a solution that should be cost-effective and easy to use.\n\n### Introducing TOTP!!!\n\nTime-Based One-Time Passwords (TOTP) are randomized numeric codes generated by authenticator apps like [Google Authenticator](https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2&hl=en) and [Microsoft Authenticator](https://play.google.com/store/apps/details?id=com.azure.authenticator&hl=en). These codes are generated based on a shared secret and a time counter, which means a new OTP is typically generated every 30 seconds. The OTP is valid only for the allotted time window, beyond which it becomes redundant, preventing OTP reuse.\n\n#### Fun Fact!\n\nThe first TOTP authentication system was developed, patented, and marketed by RSA Security. Later, the Initiative for Open Authentication (OATH) developed its own variant of TOTP, which it made freely available. I hope many of you have seen the image below 😆\n\n![dog-mfa](./images/rsa.png)\n\n<br />\nLet's see how we can implement one. I'm using C# here as an example, but it should\nbe pretty much the same across platforms.\n\nBelow is the basic user journey for registration and authentication:\n\n![mfa-flow](./images/mfa-flow.png)\n\n<br />\n\n```csharp\nusing OtpNet;\nusing QRCoder;\n```\n\n<br />\nWe are using [otp.net](https://github.com/kspearrin/Otp.NET?tab=readme-ov-file#totp-timed-one-time-password)\nand [QRCode](https://github.com/codebude/QRCoder) here\n\n```csharp\nvar secret = Base32Encoding.ToString(\n                KeyGeneration.GenerateRandomKey(20)\n                );\n```\n\n<br /> The secret is generated using `KeyGeneration.GenerateRandomKey`. I would\nprefer one secret per user and store it securely after encrypting it using AES\nor other popular algorithms.\n\n```csharp var qrCode = GenerateQrCodeUri(\"some@email.com\",secret);\n\nstring GenerateQrCodeUri(string email, string secret)\n{\nstring issuer = \"my amazing app\";\nstring uri = $\"otpauth://totp/{issuer}:{email}?secret={secret}&issuer={issuer}\";\n\n    using (var qrGenerator = new QRCodeGenerator())\n    {\n    \tvar qrCodeData = qrGenerator.CreateQrCode(uri, QRCodeGenerator.ECCLevel.Q);\n    \tvar qrCode = new Base64QRCode(qrCodeData);\n    \treturn qrCode.GetGraphic(20);\n    }\n\n}\n\n```\n\n<br /> With the help of QRCoder, we can generate a QR code. Below is the output\nfrom the above code, which you can scan using authenticator apps.\n\n![mfa-qr](./images/mfa-qr.png)\n\n<br />\n\nOnce you have scanned the QR code, it will appear in authenticator apps like the one below. Oh yeah, the screenshot below is taken from the Chrome Google Authenticator extension.\n\n![mfa-auth](./images/mfa-auth.png)\n\n<br />\n\nNow you have a time-based OTP. Let's see how to validate it.\n\n```csharp\nvar secret = \"my secure secret\";\n\nvar code = \"650260\";\n\nvar isValid = ValidateTotpCode(secret,code);\n\nisValid.Dump();\n\nbool ValidateTotpCode(string secret, string code)\n{\n\tvar totp = new Totp(Base32Encoding.ToBytes(secret));\n\treturn totp.VerifyTotp(code, out long timeStepMatched, new VerificationWindow(2, 2));\n}\n```\n\n<br />\n\n`totp.VerifyTotp` will help us validate whether the input TOTP is valid or not.\n\nTada!!! With these easy steps, you have enabled MFA for your application.\n\nHaving MFA will not protect against impersonating cookies or JWTs. We will see how to make your JWT more secure in upcoming posts.\n"},{"id":"628a6c9b-cf9c-5d20-ac2e-ba206db28545","frontmatter":{"title":"Decoding the Developer Landscape - the Stack Overflow Survey 2024","date":"July 29, 2024","tags":["tech"]},"fields":{"slug":"/stack-overflow-survey-2024/"},"body":"\n[Stack Overflow Developer Survey 2024](https://survey.stackoverflow.co/2024/) is out! Last year's survey provided insights into how developers around the world are embracing AI in their development processes. Let's find out what this year's survey has in store.\n\nAll views expressed here are opinionated and based solely on my experience.\n\n### Web Development Scene\n\n> JavaScript remains the most popular language this year.\n> Python is the first language of choice for learners.\n\nJavaScript is the language of the web, and it's here to stay. If you are a newbie developer, consider learning at least one JavaScript framework. According to the survey, **React.js** is still the number one preferred framework among professional developers worldwide. **Node.js** remains the preferred JavaScript runtime/JavaScript backend, but there is a significant shift towards **Next.js**.\n\nAt last year's Agoda Dev Day, there was a quiz question: which is older, Python or Java? Most developers jumped on Java, thinking it's the ancient beast we all know, but in reality, Python is older than Java 🐍. Sorry, Java! Once again, Python finds its way into many university curriculums, and with abundant learning content on YouTube, it's no wonder Python is the number one language for learners.\n\n![Image](./images/top-fw-so-survey-2024.png)\n\n---\n\n> Docker remains the most used tool among developers.\n\nLearning Docker is crucial for any developer. I have experienced both worlds where deployment happens with MSI installers or sometimes ZIP files deployed through Octopus Deploy, but I would choose Docker for development any day. Containers play a key role in my development experience, from test containers to production environments. Since Docker is no longer free, we use Docker in WSL for Windows and OrbStack for Mac development.\n\n#### What's the Scene on Databases?\n\n> PostgreSQL debuted in the developer survey in 2018 when 33% of developers reported using it, compared with the most popular option that year: MySQL, in use by 59% of developers. Six years later, PostgreSQL is used by 49% of developers and is the most popular database for the second year in a row.\n\nMy colleague at Agoda was showing some performance results between SQL Server and PostgreSQL for some of our use cases. The results are stunning, though it's early to comment on the final outcome. My experience with PostgreSQL has been positive. At the very least, you save a ton of money on licensing 🤣, and performance is an added advantage.\n\n#### What About the Pay of Developers Using These Technologies?\n\nJavaScript developers' salaries have gone down in the new survey. I see a problem here, With the advent of AI tools like Co-pilot, any back-end developer can comfortably code in the front end when you have a proper design system in place. Does this mean the need for pure front-end developers is decreasing? Yes, at least according to the survey.\n\n> Full-stack, back-end, and front-end developers were the top three roles reported by developers for the last three years; however, front-end developers have decreased from 6.6% to 5.6% since last year.\n\nThis survey on developer pay should be taken with a pinch of salt. Most of the languages that attract high pay are niche and not widely used. According to the survey, even the poster boys Rust and Go have seen a decrease in salary 🤣.\n\n![Image](./images/top-pay-so-survey-2024.png)\n\n---\n\n#### No insights on Testing\n\nI am concerned that Stack Overflow didn't include any survey questions on testing and deployment tools and strategies, which are highly important. Given the increased amount of code being written and deployed to production since the advent of AI tools, these topics are crucial. I hope they make it into the survey next time.\n\n#### What's the Scene on AI Tools for Development?\n\n> 76% of all respondents are using or are planning to use AI tools in their development process this year, an increase from last year (70%). Many more developers are currently using AI tools this year, too (62% vs. 44%).\n\nOne tool that deserves my love among the AI tools mentioned in the survey is **GitHub Co-pilot**. I can't stress enough how much development effort it saves for me.\n\nGitHub provides a one-month free subscription to Co-pilot. Try it out. If you are an engineering leader and you are reading this, hands down, Co-pilot improves developer experience. Happy developers lead to a better product.\n\nThis is not a product advertisement 🤭. And no, Co-pilot is not an overhyped auto-completion tool.\n\n![Image](./images/top-ai-tool-so-survey-2024.png)\n\n---\n\nAgoda is doing a ton of things using ChatGPT. I'll write a blog about it later, or I can share one if my colleagues write one.\n\n> 70% of professional developers do not perceive AI as a threat to their job.\n\nFinally, fellow developers are embracing AI and accepting the fact that it's a tool. Some inner peace ✌️\n\nI'll repeat one more time, **embrace AI**.\n\n#### Conclusion\n\nThe Stack Overflow Developer Survey 2024 provides valuable insights into the current state of the developer ecosystem. From the enduring popularity of JavaScript and the rise of Python as the first language of choice for learners, to the increasing use of Docker and the evolving role of AI tools like GitHub Co-pilot, this year's survey highlights several important trends. It's clear that AI is being embraced as a valuable asset, with 70% of professional developers not seeing it as a threat to their jobs.\n\nHowever, the survey did not cover testing and deployment tools and strategies—an oversight that is particularly significant given the increased volume of code being pushed to production with the help of AI tools. Hopefully, this will be addressed in future surveys. For more insights on methodologies, communities, and work environments, check out the full survey [here](https://survey.stackoverflow.co/2024/). In summary, the key takeaway is to embrace change and continue to evolve as a developer.\n"},{"id":"d54a4683-9eff-50bf-88bf-dae0f20854cf","frontmatter":{"title":"Your Resumes are read by machines","date":"July 22, 2024","tags":["general"]},"fields":{"slug":"/your-resumes-are-read-by-machines/"},"body":"\nIn today's competitive job market, it's not uncommon for a single job posting to receive thousands of applications. For example, Company A might receive around 2,000 applications for a single job post. Manually reviewing each resume is not feasible, so many companies turn to AI-powered CV screening tools like Greenhouse to streamline the process.\n\nThese tools use algorithms to scan and evaluate resumes based on predefined criteria, ensuring that only the most relevant candidates make it through to the next stage. However, many resumes are not structured or written in a way that these AI tools can effectively parse and evaluate. This can result in qualified candidates being overlooked simply because their resumes don't meet the machine's criteria.\n\nLet's take a closer look at how these tools work and what you can do to optimize your resume for AI screening.\n\nLets consider this resume, This is from my former collegue. All PII data redacted for obvious reasons.\nHe is a fantastic back end engineer, one of the best i have worked with, will he be part of my dream team ofcourse yes!!\n\n---\n![Image](./images/example-resume.png)\n\n---\n  \nBut machines dont understand relationship dynamics, let see how it reads.\n\nConsider the following automated review for a Senior Software Engineer position at Company A:\n\n---\n\nAutomated Review for Application ID *******, Job Pool [BE] Senior Software Engineer\n\nFeature evaluation:\n\n- Total working years -\n  5 years, Reason: Total working experience 5.25 years (including 2.25 years at ******* 0.75 years at *******, 2.25 years at *******)\n\n- Total years in last job -\n  2 years, Reason: Years in last job: 2.25 years at *******\n\n- Has any mentioned achievements, awards or recognitions -\n  Requirement Passed: False, Reason: Matches: | AdditionalInfo: The CV does not mention any of the listed achievements, awards, or recognitions.\n\n> This guy hasn't mentioned any of his achievements in the CV.\n\n- Has a job title tagged as back-end -\n  Requirement Passed: True, Reason: Found specified tag: IsBackendExperience in one of the job titles is true\n\n- Has been promoted? -\n  Requirement Passed: False, Reason: The CV does not indicate that the candidate has been promoted within the same company.\n\n> Although he was promoted, the previous titles, roles and responsibilty is not mentioned in the cv.\n\n- Has a job title tagged as big data or distributed system -\n  Requirement Passed: True, Reason: Found specified tag: isBigDataOrDistributedSystemExperience in one of the job titles is true\n\n> This is important your resume should have key words from JD\n\n- Has completed any programming competitions -\n  Requirement Passed: False, Reason: The CV does not mention any programming competitions.\n\n> Similar to achievements , these are also crucial but could be added advantage\n\n- Has experience leading a team or project -\n  Requirement Passed: False, Reason: The CV does not provide any specific examples of the candidate leading a team or a project.\n\n- Has mentorship experience -\n  Requirement Passed: False, Reason: The CV does not provide any specific examples of the candidate mentoring or leading junior engineers.\n\n- Has lead, principal or manager job titles on CV -\n  Requirement Passed: False, Reason: The candidate does not have any job titles such as 'lead engineer', 'tech lead', 'lead developer', 'principal', 'engineering manager', or 'development manager' for more than 1 year.\n\n- Is currently working at company of interest -\n  Requirement Passed: False\n\nCV summary:\n\nThe candidate has a Bachelor's degree in Computer Science from Sri Krishna College of Engineering and Technology with a CGPA of 8.6. They have over 5 years of experience in software engineering, with roles at Odessa Technologies, Thoughtworks, and Invest Voyager. The candidate has expertise in C#, .NET, SQL Server, LINQ, SSRS\n\nThe candidate has expertise in C#, .NET, SQL Server, LINQ, SSRS, Go, Kubernetes, Docker, and Websockets.\n\n---\n\n#### Understanding the Automated Review Process\nThe automated review process evaluates various features of the resume to determine if the candidate meets the job requirements. Here are some key points from the example review:\n\n- Total Working Years: The tool checks the total years of experience and breaks it down by each job.\n- Achievements and Awards: The tool looks for mentions of achievements, awards, or recognitions.\n- Job Titles and Tags: The tool checks for specific job titles and tags related to the job requirements, such as back-end, big data, or distributed systems experience.\n- Promotions: The tool looks for evidence of promotions within the same company.\n- Programming Competitions: The tool checks for mentions of participation in programming competitions.\n- Leadership and Mentorship: The tool looks for examples of leading teams or projects and mentoring junior engineers.\n- Technical Papers: The tool checks for mentions of published technical papers.\n- Language and Visa Requirements: The tool verifies if the CV is in English and if the candidate meets visa requirements.\n\n#### Optimizing Your Resume for AI Screening Tools\nTo ensure your resume is effectively parsed and evaluated by AI screening tools, consider the following tips:\n\n- Use Clear and Consistent Formatting: Ensure your resume is well-structured with clear headings and consistent formatting. Avoid using complex layouts or graphics that may confuse the parsing algorithms.\n\n- Include Relevant Keywords: Use keywords and phrases that match the job description. For example, if the job requires experience in \"big data\" or \"distributed systems,\" make sure these terms are prominently featured in your resume.\n\n- Highlight Achievements and Awards: Clearly mention any achievements, awards, or recognitions. Use bullet points to list these accomplishments and make them easy to find.\n\n- Detail Your Job Titles and Responsibilities: Provide detailed job titles and descriptions of your responsibilities. Use tags or keywords that match the job requirements, such as \"back-end,\" \"full-stack,\" or \"data engineering.\"\n\n- Showcase Leadership and Mentorship: If you have experience leading teams or mentoring junior engineers, make sure to highlight these experiences. Provide specific examples and use relevant keywords.\n\n- Mention Technical Papers and Competitions: If you have published technical papers or participated in programming competitions, include these details in your resume.\n\n- Ensure Language and Visa Compliance: Make sure your resume is written in English and clearly states your visa status if applicable.\n\n#### Conclusion\nIn today's job market, your resume is often the first point of contact with potential employers, and it's increasingly likely that it will be read by machines before it reaches human eyes. AI-powered CV screening tools like Greenhouse are used by many companies to efficiently process large volumes of applications. To ensure your resume stands out and makes it through the initial screening, it's essential to optimize it for these tools.\n\nBy using clear formatting, including relevant keywords, highlighting achievements, and providing detailed job descriptions, you can improve your chances of passing the automated review and getting noticed by hiring managers. Remember, your resume is a reflection of your professional experience and skills, so take the time to craft it carefully and make sure it effectively communicates your qualifications.\n"}]}}}